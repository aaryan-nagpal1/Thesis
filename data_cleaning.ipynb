{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6379ab54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "moderately    1432\n",
      "slightly       926\n",
      "alert          448\n",
      "very           189\n",
      "1back           81\n",
      "2back           78\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"S1_S3_CombinedData_cleaned.csv\")\n",
    "\n",
    "# # Define label mapping\n",
    "# label_map = {\n",
    "#     \"Not drowsy\": \"alert\",\n",
    "#     \"Not Drowsy\": \"alert\",\n",
    "#     \"Slightly drowsy\": \"slightly\",\n",
    "#     \"Slightly Drowsy\": \"slightly\",\n",
    "#     \"Moderately Drowsy\": \"moderately\",\n",
    "#     \"Moderately drowsy\": \"moderately\",\n",
    "#     \"Very drowsy\": \"very\",\n",
    "#     \"Very Drowsy\": \"very\"\n",
    "# }\n",
    "\n",
    "# # Apply the mapping\n",
    "# df[\"Label\"] = df[\"Label\"].replace(label_map)\n",
    "\n",
    "# Optional: View value counts for verification\n",
    "print(df[\"Label\"].value_counts())\n",
    "\n",
    "# # Save cleaned DataFrame\n",
    "# df.to_csv(\"S1_S3_CombinedData_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9f2367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 24 files → S1_combined_cleaned_windowed.csv (rows: 8249)\n"
     ]
    }
   ],
   "source": [
    "# Combine all CSVs in \"Windowed and Cleaned Data\" into S1_combined_cleaned_windowed.csv\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "folder = \"Windowed and Cleaned Data\"\n",
    "out_path = \"S1_combined_cleaned_windowed.csv\"\n",
    "\n",
    "# Expected headers (order will be enforced)\n",
    "cols = [\n",
    "    \"window_start\",\"window_end\",\"ECG_HR\",\"GSR_mean\",\"GSR_std\",\n",
    "    \"laneDev_std\",\"speed_mean\",\"speed_std\",\"swAngle_SWRR\",\"ID\",\"Label\"\n",
    "]\n",
    "\n",
    "# Find CSVs\n",
    "paths = sorted(glob.glob(os.path.join(folder, \"*.csv\")))\n",
    "if not paths:\n",
    "    raise FileNotFoundError(f\"No CSV files found in: {folder}\")\n",
    "\n",
    "frames = []\n",
    "for p in paths:\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "        # keep only known columns (and in the right order)\n",
    "        missing = set(cols) - set(df.columns)\n",
    "        if missing:\n",
    "            print(f\"Skipping {os.path.basename(p)} (missing columns: {missing})\")\n",
    "            continue\n",
    "        df = df[cols]\n",
    "        frames.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {os.path.basename(p)} (read error: {e})\")\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No valid CSVs to combine after header checks.\")\n",
    "\n",
    "combined = pd.concat(frames, axis=0, ignore_index=True)\n",
    "\n",
    "# Optional: sort by ID then time\n",
    "combined[\"window_start\"] = pd.to_datetime(combined[\"window_start\"])\n",
    "combined[\"window_end\"]   = pd.to_datetime(combined[\"window_end\"])\n",
    "combined = combined.sort_values([\"ID\", \"window_start\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "combined.to_csv(out_path, index=False)\n",
    "print(f\"Combined {len(frames)} files → {out_path} (rows: {len(combined)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810ebee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined 24 files\n",
      "→ Training set: Classification_Combined_Data/S1_train_cleaned_windowed.csv (6556 rows)\n",
      "→ Testing set: Classification_Combined_Data/S1_test_cleaned_windowed.csv (1693 rows)\n",
      "→ Full dataset: Classification_Combined_Data/S1_combined_cleaned_windowed.csv (8249 rows)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Folder and output paths\n",
    "folder = \"S1_Data/Windowed and Cleaned Data\"\n",
    "combined_out_path = \"Classification_Combined_Data/S1_combined_cleaned_windowed.csv\"\n",
    "train_out_path = \"Classification_Combined_Data/S1_train_cleaned_windowed.csv\"\n",
    "test_out_path = \"Classification_Combined_Data/S1_test_cleaned_windowed.csv\"\n",
    "\n",
    "# Feature and meta columns\n",
    "feature_cols = [\"ECG_HR\", \"GSR_mean\", \"GSR_std\", \"laneDev_std\", \"speed_mean\", \"speed_std\", \"swAngle_SWRR\"]\n",
    "meta_cols = [\"window_start\", \"window_end\", \"ID\", \"Label\"]\n",
    "all_cols = meta_cols[:2] + feature_cols + meta_cols[2:]\n",
    "\n",
    "# Locate CSVs\n",
    "paths = sorted(glob.glob(os.path.join(folder, \"*.csv\")))\n",
    "if len(paths) < 24:\n",
    "    raise ValueError(f\"Expected 24 files, found {len(paths)}\")\n",
    "\n",
    "# Shuffle and split into train/test by participant\n",
    "random.seed(42)\n",
    "random.shuffle(paths)\n",
    "train_paths = paths[:18]\n",
    "test_paths = paths[18:]\n",
    "\n",
    "def process_file(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Check column validity\n",
    "    if not set(all_cols).issubset(df.columns):\n",
    "        raise ValueError(f\"Missing columns in file {path}\")\n",
    "\n",
    "    df = df[all_cols]\n",
    "\n",
    "    # Normalize feature columns per participant\n",
    "    scaler = StandardScaler()\n",
    "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Process each group\n",
    "train_frames = []\n",
    "test_frames = []\n",
    "\n",
    "for path in train_paths:\n",
    "    try:\n",
    "        df = process_file(path)\n",
    "        train_frames.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping train file {os.path.basename(path)} due to error: {e}\")\n",
    "\n",
    "for path in test_paths:\n",
    "    try:\n",
    "        df = process_file(path)\n",
    "        test_frames.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping test file {os.path.basename(path)} due to error: {e}\")\n",
    "\n",
    "# Combine\n",
    "train_combined = pd.concat(train_frames, axis=0, ignore_index=True)\n",
    "test_combined = pd.concat(test_frames, axis=0, ignore_index=True)\n",
    "full_combined = pd.concat([train_combined, test_combined], axis=0, ignore_index=True)\n",
    "\n",
    "# Sort chronologically per participant\n",
    "for df in [train_combined, test_combined, full_combined]:\n",
    "    df[\"window_start\"] = pd.to_datetime(df[\"window_start\"])\n",
    "    df[\"window_end\"] = pd.to_datetime(df[\"window_end\"])\n",
    "    df.sort_values(by=[\"ID\", \"window_start\"], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save results\n",
    "train_combined.to_csv(train_out_path, index=False)\n",
    "test_combined.to_csv(test_out_path, index=False)\n",
    "full_combined.to_csv(combined_out_path, index=False)\n",
    "\n",
    "print(f\"✅ Combined {len(paths)} files\")\n",
    "print(f\"→ Training set: {train_out_path} ({len(train_combined)} rows)\")\n",
    "print(f\"→ Testing set: {test_out_path} ({len(test_combined)} rows)\")\n",
    "print(f\"→ Full dataset: {combined_out_path} ({len(full_combined)} rows)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
